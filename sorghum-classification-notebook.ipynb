{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## References\n\n1. Focal loss : https://towardsdatascience.com/a-loss-function-suitable-for-class-imbalanced-data-focal-loss-af1702d75d75\n2. Past plant competition : https://www.kaggle.com/competitions/plant-pathology-2021-fgvc8/discussion/242275","metadata":{}},{"cell_type":"code","source":"!pip install -qq albumentations==1.0.3\n!pip install wandb --upgrade\n!pip install timm\n!pip install torch==1.10.0\n\n# !pip install pretrainedmodels\n# !pip install --upgrade efficientnet-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:04:06.266268Z","iopub.execute_input":"2022-03-31T03:04:06.266550Z","iopub.status.idle":"2022-03-31T03:05:36.783526Z","shell.execute_reply.started":"2022-03-31T03:04:06.266518Z","shell.execute_reply":"2022-03-31T03:05:36.782522Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport functools\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport timm\n# from efficientnet_pytorch import EfficientNet\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchmetrics import AveragePrecision, Recall, F1\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport albumentations\n\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport wandb\nwandb.login()\n# b8097c5d4e834adf72ecb12daa4275aaca4acf2c","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:05:36.785693Z","iopub.execute_input":"2022-03-31T03:05:36.786296Z","iopub.status.idle":"2022-03-31T03:06:56.326378Z","shell.execute_reply.started":"2022-03-31T03:05:36.786258Z","shell.execute_reply":"2022-03-31T03:06:56.325642Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv')\nsub_csv = pd.read_csv('../input/sorghum-id-fgvc-9/sample_submission.csv')\ndata.shape, sub_csv.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:06:58.383411Z","iopub.execute_input":"2022-03-31T03:06:58.383729Z","iopub.status.idle":"2022-03-31T03:06:58.455589Z","shell.execute_reply.started":"2022-03-31T03:06:58.383670Z","shell.execute_reply":"2022-03-31T03:06:58.454876Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(list(data.cultivar.unique()))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:06:59.883450Z","iopub.execute_input":"2022-03-31T03:06:59.883991Z","iopub.status.idle":"2022-03-31T03:06:59.900859Z","shell.execute_reply.started":"2022-03-31T03:06:59.883953Z","shell.execute_reply":"2022-03-31T03:06:59.899984Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class_counts = dict(data.cultivar.value_counts())\nkeys = list(class_counts.keys())\nvalues = list(class_counts.values())\nplt.figure(figsize=(10, 20))\nsns.barplot(values, keys)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:01.153184Z","iopub.execute_input":"2022-03-31T03:07:01.154045Z","iopub.status.idle":"2022-03-31T03:07:02.803245Z","shell.execute_reply.started":"2022-03-31T03:07:01.153997Z","shell.execute_reply":"2022-03-31T03:07:02.802517Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#None will be filled later during the process.\nclass Config:\n    #general.\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    proj_name = 'Sorghum Kaggle Competition'\n    submission_file = 'submission.csv'\n    \n    #dataset params.\n    train_folder = '../input/sorghum-id-fgvc-9/train_images'\n    test_folder = '../input/sorghum-cultivar-identification-512512/test'\n    sub_folder = '../input/sorghum-id-fgvc-9/test'\n    val_percent = 0.2\n    \n    #model params.\n    model_name = 'tf_efficientnet_b4_ns' #tf_efficientnet_b0 #vit_base_patch16_224\n    img_dim = 512\n    out_features = 100\n    in_channels = 3\n    pretrained = True\n    dropout = 0.5\n    \n    #train params.\n    epochs = 10\n    batch_size = 8\n    learning_rate = 2e-5\n    label_smoothing = 0 #loss.\n    penalty = 2 #loss.\n    transform = None\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:02.804473Z","iopub.execute_input":"2022-03-31T03:07:02.804692Z","iopub.status.idle":"2022-03-31T03:07:02.855519Z","shell.execute_reply.started":"2022-03-31T03:07:02.804663Z","shell.execute_reply":"2022-03-31T03:07:02.854533Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class sorghumDataset(Dataset):\n    \n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        return (self.images[index], torch.tensor(self.labels[index]))\n    \nclass sorghumTestDataset(Dataset):\n    \n    def __init__(self, images):\n        self.images = images\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        return self.images[index]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:10.210647Z","iopub.execute_input":"2022-03-31T03:07:10.211214Z","iopub.status.idle":"2022-03-31T03:07:10.217379Z","shell.execute_reply.started":"2022-03-31T03:07:10.211177Z","shell.execute_reply":"2022-03-31T03:07:10.216692Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class sorghumModel(nn.Module):\n    \n    def __init__(self, model_name, out_features, in_channels=3, drop_prob=0.2, pretrained=True):\n        super(sorghumModel, self).__init__()\n        self.out_features = out_features\n        self.in_channels = in_channels\n        \n        #         self.model = EfficientNet.from_pretrained(model_name, \n#                                      num_classes=out_features, \n#                                      dropout_rate=drop_prob, \n#                                      image_size=Config.img_dim)\n#         self.pre_model.classifier = nn.Linear(in_features, 1024, bias=True)\n        \n        self.pre_model = timm.create_model(model_name, pretrained=pretrained, in_chans=in_channels)\n        in_features = self.pre_model.classifier.in_features #head or fc or classifier\n\n        self.pre_model.classifier = nn.Sequential(\n            nn.Linear(in_features, in_features, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(drop_prob),\n            nn.Linear(in_features, self.out_features, bias=True)\n        )\n        \n#         self.cnn_head = nn.Sequential(\n#             nn.Linear(1024, 512, bias=True),\n#             nn.ReLU(),\n#             nn.Dropout(drop_prob),\n#             nn.Linear(512, 64, bias=True),\n#             nn.ReLU(),\n#             nn.Dropout(drop_prob),\n#             nn.Linear(64, self.out_features, bias=True),\n#         )\n        \n        self.drop_layer = nn.Dropout(drop_prob)\n        \n    def forward(self, image):\n#         image_feats = self.pre_model(image)\n#         image_feats = self.drop_layer(image_feats)\n#         preds = self.cnn_head(image_feats)\n        preds = self.pre_model(image)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:11.672924Z","iopub.execute_input":"2022-03-31T03:07:11.673657Z","iopub.status.idle":"2022-03-31T03:07:11.682690Z","shell.execute_reply.started":"2022-03-31T03:07:11.673607Z","shell.execute_reply":"2022-03-31T03:07:11.681901Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#            albumentations.Blur(blur_limit=3, always_apply=False, p=0.5),\n# albumentations.Resize(DIM,DIM),\ndef train_transform_object(DIM = Config.img_dim):\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM,DIM),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1),\n                contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            albumentations.Flip(p=0.5),\n            albumentations.Rotate (limit=90, always_apply=False, p=0.5),\n            albumentations.CenterCrop (height=Config.img_dim, width=Config.img_dim, always_apply=False, p=0.5),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef valid_transform_object(DIM = 384):\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM,DIM),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef collate_fn(batch, process):\n    images, labels = [], []\n    for sample in batch:\n        im_name, im_label = sample[0], sample[1]\n        im = cv2.imread(os.path.join(Config.train_folder, im_name), 1)\n        if Config.transform is not None:\n            if process == 'training':\n                im = Config.transform['train_transform'](image=im)['image']\n            else:\n                im = Config.transform['valid_transform'](image=im)['image']\n        images.append(im)\n        labels.append(im_label)\n    images_tensor = torch.stack(images)\n    labels_tensor = torch.stack(labels)\n    return images_tensor, labels_tensor\n\ndef collate_test_fn(batch):\n    images = []\n    for sample in batch:\n        im_name = sample\n        im = cv2.imread(os.path.join(Config.test_folder, im_name), 1)\n        if Config.transform is not None:\n            im = Config.transform['valid_transform'](image=im)['image']\n        images.append(im)\n    images_tensor = torch.stack(images)\n    return images_tensor","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:13.245402Z","iopub.execute_input":"2022-03-31T03:07:13.245887Z","iopub.status.idle":"2022-03-31T03:07:13.261546Z","shell.execute_reply.started":"2022-03-31T03:07:13.245846Z","shell.execute_reply":"2022-03-31T03:07:13.260772Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py\n\nclass FocalLoss(nn.modules.loss._WeightedLoss):\n    def __init__(self, weight=None, gamma=Config.penalty,reduction='mean'):\n        super(FocalLoss, self).__init__(weight,reduction=reduction)\n        self.gamma = gamma\n        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n\n    def forward(self, input, target):\n\n        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:14.674633Z","iopub.execute_input":"2022-03-31T03:07:14.674927Z","iopub.status.idle":"2022-03-31T03:07:14.681449Z","shell.execute_reply.started":"2022-03-31T03:07:14.674898Z","shell.execute_reply":"2022-03-31T03:07:14.680655Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(train_loader, model, epoch, criterion, optimizer):\n    model.train()\n    stream = tqdm(train_loader)\n    total_loss = 0\n    images_done = 0\n    for i, (im_tensors, tar_tensors) in enumerate(stream, start=1):\n        im_tensors = im_tensors.to(Config.device, non_blocking=True)\n        tar_tensors = tar_tensors.float().view(-1, 1).squeeze(1).type(torch.LongTensor).to(Config.device, non_blocking=True)\n\n        output = model(im_tensors)\n        \n        loss = criterion(output, tar_tensors)\n        total_loss += float(loss)*len(im_tensors)\n        images_done += len(im_tensors)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return total_loss/images_done\n        \ndef validate(val_loader, model, epoch, criterion):\n    model.eval()\n    stream = tqdm(val_loader)\n    final_targets = []\n    final_outputs = []\n    total_loss = 0\n    images_done = 0\n    with torch.no_grad():\n        for i, (im_tensors, tar_tensors) in enumerate(stream, start=1):\n            im_tensors = im_tensors.to(Config.device, non_blocking=True)\n            tar_tensors = tar_tensors.float().view(-1, 1).squeeze(1).type(torch.LongTensor).to(Config.device, non_blocking=True)\n    \n            output = model(im_tensors)\n\n            loss = criterion(output, tar_tensors)\n            total_loss += float(loss)*len(im_tensors)\n            images_done += len(im_tensors)\n            \n            target = (tar_tensors.detach().cpu().numpy()).tolist()\n            output = (output.detach().cpu().numpy()).tolist()\n            \n            final_targets.extend(target)\n            final_outputs.extend(output)\n\n    return total_loss/images_done, torch.tensor(final_targets), torch.tensor(final_outputs)\n\ndef test(model, test_loader, id_class):\n    sub_csv = pd.read_csv('../input/sorghum-id-fgvc-9/sample_submission.csv')\n    \n    model.eval()\n    stream = tqdm(test_loader)\n    final_outputs = []\n    with torch.no_grad():\n        for i, im_tensors in enumerate(stream, start=1):\n            im_tensors = im_tensors.to(Config.device, non_blocking=True)\n            output = model(im_tensors)            \n            output = (output.detach().cpu().numpy()).tolist()\n            final_outputs.extend(output)\n    \n    classes = cvt_classes(torch.tensor(final_outputs))\n    sub_csv['cultivar'] = classes\n    sub_csv['cultivar'] = sub_csv['cultivar'].map(id_class)\n    return sub_csv","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:16.504674Z","iopub.execute_input":"2022-03-31T03:07:16.505232Z","iopub.status.idle":"2022-03-31T03:07:16.521672Z","shell.execute_reply.started":"2022-03-31T03:07:16.505197Z","shell.execute_reply":"2022-03-31T03:07:16.520914Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def remove_missing_images(data):\n    images = data['image'].values\n    indices = []\n    for i in range(data.shape[0]):\n        im = data.image.iloc[i]\n        if not os.path.exists(os.path.join(Config.train_folder, im)):\n            indices.append(i)\n    data = data.drop(indices, axis=0).reset_index(drop=True)\n    return data\n\ndef setup_training(hyp):\n    data = pd.read_csv('../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv')\n    data = remove_missing_images(data)\n    \n    classes = list(data.cultivar.unique())\n    id_class = dict([(k, v) for k, v in enumerate(classes)])\n    class_id = dict([(v, k) for k, v in id_class.items()])\n    data['cultivar'] = data['cultivar'].map(class_id)\n\n    images, labels = data.image.values, data.cultivar.values\n    train_data, val_data, train_labels, val_labels = train_test_split(images, \n                                                                      labels,\n                                                                      test_size=Config.val_percent, \n                                                                      stratify=labels,\n                                                                      random_state=42,\n                                                                      shuffle=True)\n    \n    train_dataset = sorghumDataset(train_data, train_labels)\n    val_dataset = sorghumDataset(val_data, val_labels)\n    \n    train_collate = functools.partial(collate_fn, process='training')\n    valid_collate = functools.partial(collate_fn, process='validation')\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=Config.batch_size, collate_fn=train_collate)\n    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=Config.batch_size, collate_fn=valid_collate)\n    \n    model = sorghumModel(model_name=hyp.model_name, \n                         out_features=hyp.out_features, \n                         in_channels=hyp.in_channels, \n                         drop_prob=hyp.drop_prob,\n                         pretrained=hyp.pretrained)\n    model.to(Config.device)\n#     loss_fn = nn.CrossEntropyLoss(label_smoothing=hyp.label_smoothing)\n    loss_fn = FocalLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=hyp.lr_rate, weight_decay=1e-6, amsgrad=False)\n    \n    return id_class, model, train_dataloader, val_dataloader, loss_fn, optimizer\n\ndef setup_testing():\n    test_data = pd.read_csv('../input/sorghum-id-fgvc-9/sample_submission.csv')\n    test_images = test_data.filename.values\n    test_dataset = sorghumTestDataset(test_images)\n    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=Config.batch_size, collate_fn=collate_test_fn)\n    return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:26.070529Z","iopub.execute_input":"2022-03-31T03:07:26.070898Z","iopub.status.idle":"2022-03-31T03:07:26.085080Z","shell.execute_reply.started":"2022-03-31T03:07:26.070779Z","shell.execute_reply":"2022-03-31T03:07:26.083998Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def cvt_classes(outputs):\n    outputs = torch.nn.functional.softmax(outputs)\n    outputs = outputs.detach().cpu().numpy()\n    outputs = outputs.argmax(axis=1)\n    return torch.tensor(outputs).int()\n\ndef train(model, tr_loader, val_loader, criterion, optimizer, params):\n    wandb.watch(model, criterion, log='all', log_freq=10)\n    for epoch in range(params.epochs):\n        train_loss = train_one_epoch(tr_loader, model, epoch, criterion, optimizer)\n        val_loss, targets, predictions = validate(val_loader, model, epoch, criterion)\n        outputs = cvt_classes(predictions)\n        accuracy = accuracy_score(outputs, targets) \n        wandb.log({ 'epoch' : epoch, 'train_loss' : train_loss, 'val_loss' : val_loss, 'accuracy' : accuracy }, step=epoch)\n        torch.save(model.state_dict(), '{}_epoch{}.pth'.format(params.model_name, epoch))\n\ndef model_pipeline(train_parameters):\n    with wandb.init(project=Config.proj_name, config=train_parameters):\n        parameters = wandb.config\n        id_class, model, train_loader, val_loader, criterion, optimizer = setup_training(parameters)\n        train(model, train_loader, val_loader, criterion, optimizer, parameters)\n        test_loader = setup_testing()\n        result = test(model, test_loader, id_class)\n        result.to_csv(Config.submission_file, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:26.927678Z","iopub.execute_input":"2022-03-31T03:07:26.928519Z","iopub.status.idle":"2022-03-31T03:07:26.939255Z","shell.execute_reply.started":"2022-03-31T03:07:26.928479Z","shell.execute_reply":"2022-03-31T03:07:26.938112Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_params = {\n    'model_name' : Config.model_name,\n    'out_features' : Config.out_features,\n    'in_channels' : Config.in_channels,\n    'drop_prob' : Config.dropout,\n    'pretrained' : Config.pretrained,\n    'epochs' : Config.epochs,\n    'lr_rate' : Config.learning_rate,\n    'label_smoothing' : Config.label_smoothing\n}\n\nConfig.transform = {\n    'train_transform' : train_transform_object(),\n    'valid_transform' : valid_transform_object()\n}\n\nmodel_pipeline(train_params)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T03:07:27.600302Z","iopub.execute_input":"2022-03-31T03:07:27.601079Z","iopub.status.idle":"2022-03-31T09:36:02.677690Z","shell.execute_reply.started":"2022-03-31T03:07:27.601035Z","shell.execute_reply":"2022-03-31T09:36:02.676863Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/sorghum-id-fgvc-9/train_cultivar_mapping.csv')\n# x = remove_missing_images(data)\nclasses = list(data.cultivar.unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-30T15:40:34.308668Z","iopub.execute_input":"2022-03-30T15:40:34.308974Z","iopub.status.idle":"2022-03-30T15:40:34.335033Z","shell.execute_reply.started":"2022-03-30T15:40:34.308931Z","shell.execute_reply":"2022-03-30T15:40:34.334365Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data.shape, x.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-30T15:40:35.462390Z","iopub.execute_input":"2022-03-30T15:40:35.462955Z","iopub.status.idle":"2022-03-30T15:40:35.470275Z","shell.execute_reply.started":"2022-03-30T15:40:35.462910Z","shell.execute_reply":"2022-03-30T15:40:35.468506Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"len(classes)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T15:40:37.830269Z","iopub.execute_input":"2022-03-30T15:40:37.830811Z","iopub.status.idle":"2022-03-30T15:40:37.837064Z","shell.execute_reply.started":"2022-03-30T15:40:37.830772Z","shell.execute_reply":"2022-03-30T15:40:37.836185Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#label smoothing. https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186\n\nclass LabelSmoothingLoss(nn.Module):\n    \"\"\"\n    With label smoothing,\n    KL-divergence between q_{smoothed ground truth prob.}(w)\n    and p_{prob. computed by model}(w) is minimized.\n    \"\"\"\n    def __init__(self, label_smoothing, tgt_vocab_size, ignore_index=-100):\n        assert 0.0 < label_smoothing <= 1.0\n        self.ignore_index = ignore_index\n        super(LabelSmoothingLoss, self).__init__()\n\n        smoothing_value = label_smoothing / (tgt_vocab_size - 2)\n        one_hot = torch.full((tgt_vocab_size,), smoothing_value)\n        one_hot[self.ignore_index] = 0\n        self.register_buffer('one_hot', one_hot.unsqueeze(0))\n\n        self.confidence = 1.0 - label_smoothing\n\n    def forward(self, output, target):\n        \"\"\"\n        output (FloatTensor): batch_size x n_classes\n        target (LongTensor): batch_size\n        \"\"\"\n        model_prob = self.one_hot.repeat(target.size(0), 1)\n        model_prob.scatter_(1, target.unsqueeze(1), self.confidence)\n        model_prob.masked_fill_((target == self.ignore_index).unsqueeze(1), 0)\n\n        return F.kl_div(output, model_prob, reduction='sum')\n\n#reduce image size.\n#train for longer.\n#increase deapth.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}